{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is from the [blog post by @mohammed97ashraf](https://medium.com/@mohammed97ashraf/building-a-retrieval-augmented-generation-rag-model-with-gemma-and-langchain-a-step-by-step-f917fc6f753f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture piplog\n",
    "# !pip install bitsandbytes==0.42.0\n",
    "# !pip install peft==0.8.2\n",
    "# !pip install trl==0.7.10\n",
    "# !pip install accelerate==0.27.1\n",
    "# !pip install datasets==2.17.0\n",
    "# !pip install transformers==4.38.1\n",
    "# !pip install langchain sentence-transformers chromadb langchainhub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pip.log\", \"w\") as file:\n",
    "    file.write(piplog.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to .cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the repository ID for the Gemma 2b model\n",
    "repo_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# Set up a Hugging Face Endpoint for Gemma 2b model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, \n",
    "    max_new_tokens=512, \n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who won the FIFA World Cup in the year 1994?', 'text': '\\n\\nThe year 1994 was not a FIFA World Cup year, so there was no winner.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994?\"\n",
    "print(llm_chain.invoke(input={\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/google/gemma-2b-it (Request ID: 1wZeyODGR84j6n8_lGlZw)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "\n",
    "questions = [\n",
    "    {'question': \"What is Kaggle?\"},\n",
    "    {'question': \"What is the first step I should take on Kaggle?\"},\n",
    "    {'question': \"I followed your instructions. What should I do next?\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    res = llm_chain.generate(input_list=questions)\n",
    "    print(res.generations)\n",
    "except HfHubHTTPError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kaggle\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: Kaggle is a platform for data science and machine learning competitions, where users can find and publish datasets, explore and build models in a web-based data science environment, and work with other data scientists and machine learning engineers. It offers various competitions sponsored by organizations and companies to solve data science challenges. Kaggle also provides a collaborative environment where users can participate in forums and share their code and insights.\n",
    "\n",
    "Question: Which platform provides datasets, machine learning competitions, and a collaborative environment for data scientists?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the FewShotPromptTemplate class from langchain module\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# Define examples that include user queries and AI's answers specific to Kaggle competitions\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I start with Kaggle competitions?\",\n",
    "        \"answer\": \"Start by picking a competition that interests you and suits your skill level. Don't worry about winning; focus on learning and improving your skills.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What should I do if my model isn't performing well?\",\n",
    "        \"answer\": \"It's all part of the process! Try exploring different models, tuning your hyperparameters, and don't forget to check the forums for tips from other Kagglers.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How can I find a team to join on Kaggle?\",\n",
    "        \"answer\": \"Check out the competition's discussion forums. Many teams look for members there, or you can post your own interest in joining a team. It's a great way to learn from others and share your skills.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the format for how each example should be presented in the prompt\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# Create an instance of PromptTemplate for formatting the examples\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['query', 'answer'],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Define the prefix to introduce the context of the conversation examples\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI assistant focused on Kaggle competitions.\n",
    "The assistant is typically informative and encouraging, providing insightful and motivational responses to the user's questions about Kaggle. Here are some examples:\n",
    "\"\"\"\n",
    "\n",
    "# Define the suffix that specifies the format for presenting the new query to the AI\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# Create an instance of FewShotPromptTemplate with the defined examples, templates, and formatting\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "query = \"Is participating in Kaggle competitions worth my time?\"\n",
    "\n",
    "print(llm.invoke(few_shot_prompt_template.format(query=query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%. It's a fantastic opportunity to learn, network, and build your resume. Plus, the competition can be a lot of fun!\n",
      "\n",
      "These are just a few examples of the kind of responses the AI assistant provides.\n",
      "\n",
      "Based on these excerpts, what are some key takeaways for someone interested in participating in Kaggle competitions?\n",
      "\n",
      "**Key takeaways:**\n",
      "\n",
      "* **Start with a passion project:** Choose a competition that aligns with your interests and skill level.\n",
      "* **Focus on learning and improving:** Don't get caught up in the pressure of winning.\n",
      "* **Explore different approaches:** Try various models, hyperparameters, and techniques.\n",
      "* **Seek help and collaborate:** Join a team or seek advice from the Kaggle community.\n",
      "* **It's a rewarding and enriching experience:** Kaggle can significantly enhance your skills and career prospects.\n"
     ]
    }
   ],
   "source": [
    "# Import the FewShotPromptTemplate class from langchain module\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# Define examples that include user queries and AI's answers specific to Kaggle competitions\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I start with Kaggle competitions?\",\n",
    "        \"answer\": \"Start by picking a competition that interests you and suits your skill level. Don't worry about winning; focus on learning and improving your skills.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What should I do if my model isn't performing well?\",\n",
    "        \"answer\": \"It's all part of the process! Try exploring different models, tuning your hyperparameters, and don't forget to check the forums for tips from other Kagglers.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How can I find a team to join on Kaggle?\",\n",
    "        \"answer\": \"Check out the competition's discussion forums. Many teams look for members there, or you can post your own interest in joining a team. It's a great way to learn from others and share your skills.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the format for how each example should be presented in the prompt\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# Create an instance of PromptTemplate for formatting the examples\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['query', 'answer'],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Define the prefix to introduce the context of the conversation examples\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI assistant focused on Kaggle competitions.\n",
    "The assistant is typically informative and encouraging, providing insightful and motivational responses to the user's questions about Kaggle. Here are some examples:\n",
    "\"\"\"\n",
    "\n",
    "# Define the suffix that specifies the format for presenting the new query to the AI\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# Create an instance of FewShotPromptTemplate with the defined examples, templates, and formatting\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "query = \"Is participating in Kaggle competitions worth my time?\"\n",
    "\n",
    "print(llm.invoke(few_shot_prompt_template.format(query=query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
